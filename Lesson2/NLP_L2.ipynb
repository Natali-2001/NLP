{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bVAYrSpz5qzz"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import nltk\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","import warnings \n","warnings.filterwarnings(\"ignore\")\n","import os\n","\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJCY-6SZ5qz3"},"outputs":[],"source":["combine_df = pd.read_pickle('combine_df.pkl')"]},{"cell_type":"markdown","metadata":{"id":"zss-qKQM5qz4"},"source":["#### 1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). \n","Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n","\n","●\tИгнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n","\n","●\tОграничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n","\n","●\tИсключим стоп-слова с помощью stop_words='english'. \n","\n","●\tОтобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VB1VOZHD5qz_","outputId":"eb3f48b6-896e-4345-e02d-c9d7ce815531"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","      <th>tweet_new</th>\n","      <th>tweet_token</th>\n","      <th>tweet_token_filtered</th>\n","      <th>tweet_stemmed</th>\n","      <th>tweet_lemmatized</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>when a father is dysfunctional and is so sel...</td>\n","      <td>when father is dysfunctional and is so selfish...</td>\n","      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n","      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n","      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n","      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>thanks for #lyft credit i can't use cause th...</td>\n","      <td>thanks for lyft credit cannot use cause they d...</td>\n","      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n","      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n","      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n","      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>bihday your majesty</td>\n","      <td>bihday your majesty</td>\n","      <td>[bihday, your, majesty]</td>\n","      <td>[bihday, majesty]</td>\n","      <td>[bihday, majesti]</td>\n","      <td>[bihday, majesty]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","      <td>model love you take with you all the time in ur</td>\n","      <td>[model, love, you, take, with, you, all, the, ...</td>\n","      <td>[model, love, take, time, ur]</td>\n","      <td>[model, love, take, time, ur]</td>\n","      <td>[model, love, take, time, ur]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.0</td>\n","      <td>factsguide: society now    #motivation</td>\n","      <td>factsguide society now motivation</td>\n","      <td>[factsguide, society, now, motivation]</td>\n","      <td>[factsguide, society, motivation]</td>\n","      <td>[factsguid, societi, motiv]</td>\n","      <td>[factsguide, society, motivation]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  label                                              tweet  \\\n","0   1    0.0    when a father is dysfunctional and is so sel...   \n","1   2    0.0    thanks for #lyft credit i can't use cause th...   \n","2   3    0.0                                bihday your majesty   \n","3   4    0.0  #model   i love u take with u all the time in ...   \n","4   5    0.0             factsguide: society now    #motivation   \n","\n","                                           tweet_new  \\\n","0  when father is dysfunctional and is so selfish...   \n","1  thanks for lyft credit cannot use cause they d...   \n","2                                bihday your majesty   \n","3    model love you take with you all the time in ur   \n","4                  factsguide society now motivation   \n","\n","                                         tweet_token  \\\n","0  [when, father, is, dysfunctional, and, is, so,...   \n","1  [thanks, for, lyft, credit, can, not, use, cau...   \n","2                            [bihday, your, majesty]   \n","3  [model, love, you, take, with, you, all, the, ...   \n","4             [factsguide, society, now, motivation]   \n","\n","                                tweet_token_filtered  \\\n","0  [father, dysfunctional, selfish, drags, kids, ...   \n","1  [thanks, lyft, credit, use, cause, offer, whee...   \n","2                                  [bihday, majesty]   \n","3                      [model, love, take, time, ur]   \n","4                  [factsguide, society, motivation]   \n","\n","                                       tweet_stemmed  \\\n","0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n","1  [thank, lyft, credit, use, caus, offer, wheelc...   \n","2                                  [bihday, majesti]   \n","3                      [model, love, take, time, ur]   \n","4                        [factsguid, societi, motiv]   \n","\n","                                    tweet_lemmatized  \n","0  [father, dysfunctional, selfish, drag, kid, dy...  \n","1  [thanks, lyft, credit, use, cause, offer, whee...  \n","2                                  [bihday, majesty]  \n","3                      [model, love, take, time, ur]  \n","4                  [factsguide, society, motivation]  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["combine_df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DCqMlcKB5q0B","outputId":"d37270dc-9269-49d1-e586-1d52379d95f8"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abl</th>\n","      <th>absolut</th>\n","      <th>accept</th>\n","      <th>account</th>\n","      <th>act</th>\n","      <th>action</th>\n","      <th>actor</th>\n","      <th>actual</th>\n","      <th>ad</th>\n","      <th>adapt</th>\n","      <th>...</th>\n","      <th>yeah</th>\n","      <th>year</th>\n","      <th>yesterday</th>\n","      <th>yo</th>\n","      <th>yoga</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>youtub</th>\n","      <th>yr</th>\n","      <th>yummi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 1000 columns</p>\n","</div>"],"text/plain":["   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n","0    0        0       0        0    0       0      0       0   0      0  ...   \n","1    0        0       0        0    0       0      0       0   0      0  ...   \n","2    0        0       0        0    0       0      0       0   0      0  ...   \n","3    0        0       0        0    0       0      0       0   0      0  ...   \n","4    0        0       0        0    0       0      0       0   0      0  ...   \n","\n","   yeah  year  yesterday  yo  yoga  york  young  youtub  yr  yummi  \n","0     0     0          0   0     0     0      0       0   0      0  \n","1     0     0          0   0     0     0      0       0   0      0  \n","2     0     0          0   0     0     0      0       0   0      0  \n","3     0     0          0   0     0     0      0       0   0      0  \n","4     0     0          0   0     0     0      0       0   0      0  \n","\n","[5 rows x 1000 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["count_vectorizer_stemmed = CountVectorizer(max_df=0.9, \n","                                           max_features = 1000, \n","                                           stop_words='english', \n","                                           binary=False)\n","\n","bag_of_words_stemmed = count_vectorizer_stemmed.fit_transform(\n","    combine_df.tweet_stemmed.apply(lambda x: ' '.join([word for word in x]))\n",")\n","\n","feature_names = count_vectorizer_stemmed.get_feature_names()\n","\n","df_bag_of_words_stemmed = pd.DataFrame(bag_of_words_stemmed.toarray(), columns=feature_names)\n","df_bag_of_words_stemmed.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHSWwd7A5q0G","outputId":"9c1d450b-9911-46de-ec18-ef3b38ea832f"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 49159 entries, 0 to 49158\n","Columns: 1000 entries, abl to yummi\n","dtypes: int64(1000)\n","memory usage: 375.1 MB\n"]}],"source":["df_bag_of_words_stemmed.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eXFHAdD5q0H","outputId":"6e639cd5-4a4a-40f5-8e13-f2bf375e0d75"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>able</th>\n","      <th>absolutely</th>\n","      <th>account</th>\n","      <th>act</th>\n","      <th>action</th>\n","      <th>actor</th>\n","      <th>actually</th>\n","      <th>adapt</th>\n","      <th>add</th>\n","      <th>adventure</th>\n","      <th>...</th>\n","      <th>year</th>\n","      <th>yes</th>\n","      <th>yesterday</th>\n","      <th>yo</th>\n","      <th>yoga</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>youtube</th>\n","      <th>yr</th>\n","      <th>yummy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 1000 columns</p>\n","</div>"],"text/plain":["   able  absolutely  account  act  action  actor  actually  adapt  add  \\\n","0     0           0        0    0       0      0         0      0    0   \n","1     0           0        0    0       0      0         0      0    0   \n","2     0           0        0    0       0      0         0      0    0   \n","3     0           0        0    0       0      0         0      0    0   \n","4     0           0        0    0       0      0         0      0    0   \n","\n","   adventure  ...  year  yes  yesterday  yo  yoga  york  young  youtube  yr  \\\n","0          0  ...     0    0          0   0     0     0      0        0   0   \n","1          0  ...     0    0          0   0     0     0      0        0   0   \n","2          0  ...     0    0          0   0     0     0      0        0   0   \n","3          0  ...     0    0          0   0     0     0      0        0   0   \n","4          0  ...     0    0          0   0     0     0      0        0   0   \n","\n","   yummy  \n","0      0  \n","1      0  \n","2      0  \n","3      0  \n","4      0  \n","\n","[5 rows x 1000 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["count_vectorizer_lemmatized = CountVectorizer(max_df=0.9, \n","                                             max_features = 1000, \n","                                              stop_words='english', \n","                                              binary=False)\n","\n","bag_of_words_lemmatized = count_vectorizer_lemmatized.fit_transform(\n","    combine_df.tweet_lemmatized.apply(lambda x: ' '.join([word for word in x]))\n",")\n","\n","feature_names = count_vectorizer_lemmatized.get_feature_names()\n","\n","df_bag_of_words_lemmatized = pd.DataFrame(bag_of_words_lemmatized.toarray(), columns=feature_names)\n","df_bag_of_words_lemmatized.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxkGaL3x5q0I","outputId":"05fa4e3c-a8b8-4a09-d3c5-2a4dab81f95d"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 49159 entries, 0 to 49158\n","Columns: 1000 entries, able to yummy\n","dtypes: int64(1000)\n","memory usage: 375.1 MB\n"]}],"source":["df_bag_of_words_lemmatized.info()"]},{"cell_type":"markdown","metadata":{"id":"JbSExi1x5q0J"},"source":["#### 2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). \n","Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n","\n","●\tИгнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n","\n","●\tОграничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n","\n","●\tИсключим стоп-слова с помощью stop_words='english'.\n","\n","●\tОтобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KlzRzT45q0N","outputId":"c194864c-6454-4699-ef30-3ab616ab1c37"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abl</th>\n","      <th>absolut</th>\n","      <th>accept</th>\n","      <th>account</th>\n","      <th>act</th>\n","      <th>action</th>\n","      <th>actor</th>\n","      <th>actual</th>\n","      <th>ad</th>\n","      <th>adapt</th>\n","      <th>...</th>\n","      <th>yeah</th>\n","      <th>year</th>\n","      <th>yesterday</th>\n","      <th>yo</th>\n","      <th>yoga</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>youtub</th>\n","      <th>yr</th>\n","      <th>yummi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 1000 columns</p>\n","</div>"],"text/plain":["   abl  absolut  accept  account  act  action  actor  actual   ad  adapt  ...  \\\n","0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n","1  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n","2  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n","3  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n","4  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n","\n","   yeah  year  yesterday   yo  yoga  york  young  youtub   yr  yummi  \n","0   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n","1   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n","2   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n","3   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n","4   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n","\n","[5 rows x 1000 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tfidf_vectorizer_stemmed = TfidfVectorizer(max_df=0.9, \n","                                           max_features = 1000, \n","                                           stop_words='english')\n","\n","values_stemmed = tfidf_vectorizer_stemmed.fit_transform(\n","    combine_df.tweet_stemmed.apply(lambda x: ' '.join([word for word in x]))\n",")\n","\n","feature_names = tfidf_vectorizer_stemmed.get_feature_names()\n","\n","df_tfidf_vectorizer_stemmed = pd.DataFrame(values_stemmed.toarray(), columns = feature_names)\n","df_tfidf_vectorizer_stemmed.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqWGojlW5q0T","outputId":"d14dc4e7-8b30-43b2-982e-5d7cfcc55427"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 49159 entries, 0 to 49158\n","Columns: 1000 entries, abl to yummi\n","dtypes: float64(1000)\n","memory usage: 375.1 MB\n"]}],"source":["df_tfidf_vectorizer_stemmed.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIzVEnoB5q0U","outputId":"cdfb5a09-6edd-43e1-e99f-f6d1abef6c10"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>able</th>\n","      <th>absolutely</th>\n","      <th>account</th>\n","      <th>act</th>\n","      <th>action</th>\n","      <th>actor</th>\n","      <th>actually</th>\n","      <th>adapt</th>\n","      <th>add</th>\n","      <th>adventure</th>\n","      <th>...</th>\n","      <th>year</th>\n","      <th>yes</th>\n","      <th>yesterday</th>\n","      <th>yo</th>\n","      <th>yoga</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>youtube</th>\n","      <th>yr</th>\n","      <th>yummy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 1000 columns</p>\n","</div>"],"text/plain":["   able  absolutely  account  act  action  actor  actually  adapt  add  \\\n","0   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n","1   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n","2   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n","3   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n","4   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n","\n","   adventure  ...  year  yes  yesterday   yo  yoga  york  young  youtube   yr  \\\n","0        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n","1        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n","2        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n","3        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n","4        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n","\n","   yummy  \n","0    0.0  \n","1    0.0  \n","2    0.0  \n","3    0.0  \n","4    0.0  \n","\n","[5 rows x 1000 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["tfidf_vectorizer_lemmatized = TfidfVectorizer(max_df=0.9, \n","                                              max_features = 1000, \n","                                              stop_words='english')\n","\n","values_lemmatized = tfidf_vectorizer_lemmatized.fit_transform(\n","    combine_df.tweet_lemmatized.apply(lambda x: ' '.join([word for word in x]))\n",")\n","\n","feature_names = tfidf_vectorizer_lemmatized.get_feature_names()\n","\n","df_tfidf_vectorizer_lemmatized = pd.DataFrame(values_lemmatized.toarray(), columns = feature_names)\n","df_tfidf_vectorizer_lemmatized.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4-ugF1d5q0V","outputId":"c23aeba3-e416-4d27-e312-b2b396eb0966"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 49159 entries, 0 to 49158\n","Columns: 1000 entries, able to yummy\n","dtypes: float64(1000)\n","memory usage: 375.1 MB\n"]}],"source":["df_tfidf_vectorizer_lemmatized.info()"]},{"cell_type":"markdown","metadata":{"id":"djofRklw5q0W"},"source":["#### 3. Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора) обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, так же попробуйте применить к векторайзерам PCA для сокращения размерности посмотрите на качество сделайте выводы"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1n0jl2gT5q0X","outputId":"ba5719ce-e701-402d-acd3-53a4067f6802"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Stuning even for the non-gamer: This sound tra...</td>\n","      <td>__label__2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The best soundtrack ever to anything.: I'm rea...</td>\n","      <td>__label__2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Amazing!: This soundtrack is my favorite music...</td>\n","      <td>__label__2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Excellent Soundtrack: I truly like this soundt...</td>\n","      <td>__label__2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n","      <td>__label__2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text       label\n","0  Stuning even for the non-gamer: This sound tra...  __label__2\n","1  The best soundtrack ever to anything.: I'm rea...  __label__2\n","2  Amazing!: This soundtrack is my favorite music...  __label__2\n","3  Excellent Soundtrack: I truly like this soundt...  __label__2\n","4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["corpus = open('corpus').read()\n","labels, texts = [], []\n","for i, line in enumerate(corpus.split(\"\\n\")):\n","    content = line.split()\n","    labels.append(content[0])\n","    texts.append(\" \".join(content[1:]))\n","\n","# создаем df\n","corpus_df = pd.DataFrame()\n","corpus_df['text'] = texts\n","corpus_df['label'] = labels\n","\n","corpus_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXH-XLHA5q0Y"},"outputs":[],"source":["from sklearn import model_selection, preprocessing, linear_model\n","from sklearn.metrics import accuracy_score\n","from sklearn.decomposition import TruncatedSVD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQbHws_N5q0Z"},"outputs":[],"source":["x_train, x_valid, y_train, y_valid = model_selection.train_test_split(corpus_df['text'],\n","                                                                      corpus_df['label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgaVuK4q5q0Z"},"outputs":[],"source":["encoder = preprocessing.LabelEncoder()\n","\n","y_train = encoder.fit_transform(y_train)\n","y_valid = encoder.fit_transform(y_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6L-3tud5q0a"},"outputs":[],"source":["count_vect_corp = CountVectorizer(analyzer='word', \n","                                  token_pattern=r'\\w{1,}')\n","\n","tfidf_vect_corp = TfidfVectorizer(analyzer='word', \n","                                 token_pattern=r'\\w{1,}')\n","\n","count_vect_corp_mod = CountVectorizer(max_df=0.9, \n","                                      max_features = 1000, \n","                                      stop_words='english', \n","                                      analyzer='word', \n","                                      token_pattern=r'\\w{1,}')\n","\n","tfidf_vect_corp_mod = TfidfVectorizer(max_df=0.9, \n","                                     max_features = 1000, \n","                                     stop_words='english', \n","                                     analyzer='word', \n","                                     token_pattern=r'\\w{1,}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKw0-KLK5q0a","outputId":"81f428c0-4d1f-4cfd-b7ee-86fea26016d0"},"outputs":[{"data":{"text/plain":["TfidfVectorizer(max_df=0.9, max_features=1000, stop_words='english',\n","                token_pattern='\\\\w{1,}')"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["count_vect_corp.fit(corpus_df['text'])\n","tfidf_vect_corp.fit(corpus_df['text'])\n","count_vect_corp_mod.fit(corpus_df['text'])\n","tfidf_vect_corp_mod.fit(corpus_df['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzhvsMNk5q0b"},"outputs":[],"source":["res = pd.DataFrame()\n","\n","vectorizers = [count_vect_corp,\n","               tfidf_vect_corp, \n","                       \n","               count_vect_corp_mod, \n","               tfidf_vect_corp_mod, \n","                       \n","               count_vectorizer_stemmed, \n","               tfidf_vectorizer_stemmed, \n","                       \n","               count_vectorizer_lemmatized, \n","               tfidf_vectorizer_lemmatized]\n","\n","for vectorizer in vectorizers:\n","\n","    x_train_count = vectorizer.transform(x_train)\n","    x_valid_count = vectorizer.transform(x_valid)\n","\n","    classifier = linear_model.LogisticRegression()    \n","    classifier.fit(x_train_count, y_train)\n","    \n","    predictions = classifier.predict(x_valid_count)\n","    \n","    res = res.append(\n","        pd.DataFrame([[str(vectorizer), accuracy_score(y_valid, predictions)]], columns=['Vectorizer', 'Score'])\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7wrcDH25q0c","outputId":"b26a3cbf-7345-44e5-8402-dac7223a52aa"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Vectorizer</th>\n","      <th>Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TfidfVectorizer(token_pattern='\\\\w{1,}')</td>\n","      <td>0.8724</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>CountVectorizer(token_pattern='\\\\w{1,}')</td>\n","      <td>0.8692</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>TfidfVectorizer(max_df=0.9, max_features=1000,...</td>\n","      <td>0.8440</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>CountVectorizer(max_df=0.9, max_features=1000,...</td>\n","      <td>0.8276</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>TfidfVectorizer(max_df=0.9, max_features=1000,...</td>\n","      <td>0.7808</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>CountVectorizer(max_df=0.9, max_features=1000,...</td>\n","      <td>0.7772</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>TfidfVectorizer(max_df=0.9, max_features=1000,...</td>\n","      <td>0.7584</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>CountVectorizer(max_df=0.9, max_features=1000,...</td>\n","      <td>0.7508</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          Vectorizer   Score\n","0           TfidfVectorizer(token_pattern='\\\\w{1,}')  0.8724\n","0           CountVectorizer(token_pattern='\\\\w{1,}')  0.8692\n","0  TfidfVectorizer(max_df=0.9, max_features=1000,...  0.8440\n","0  CountVectorizer(max_df=0.9, max_features=1000,...  0.8276\n","0  TfidfVectorizer(max_df=0.9, max_features=1000,...  0.7808\n","0  CountVectorizer(max_df=0.9, max_features=1000,...  0.7772\n","0  TfidfVectorizer(max_df=0.9, max_features=1000,...  0.7584\n","0  CountVectorizer(max_df=0.9, max_features=1000,...  0.7508"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["res.sort_values('Score', ascending=False, inplace=True)\n","res.head(10)"]},{"cell_type":"markdown","metadata":{"id":"BNEvn5ZD5q0e"},"source":["Самый лучший результат показывает CountVectorizer с разбиением по паттерну, обученный на корпусе. \n","\n","Обученные на твитах векторайзеры показывают более низкий результат. "]},{"cell_type":"markdown","metadata":{"id":"COA6nDs35q0f"},"source":["### PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MrXN8LKh5q0h","outputId":"ef726bc6-32aa-4ed9-80a1-ca4d4882f1f6"},"outputs":[{"data":{"text/plain":["(10000, 31681)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["values = tfidf_vect_corp.fit_transform(corpus_df['text'])\n","values.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGPbf7ZD5q0s"},"outputs":[],"source":["res = pd.DataFrame()\n","\n","for n_components in [100, 300, 500, 1000, 3000, 5000]:\n","    svd = TruncatedSVD(n_components=n_components)\n","    \n","    x_train_count_2 =  count_vect_corp.transform(x_train)\n","    x_valid_count_2 =  count_vect_corp.transform(x_valid)\n","\n","    x_train_count_pca =  svd.fit_transform(x_train_count_2)\n","    x_valid_count_pca =  svd.transform(x_valid_count_2)\n","\n","    classifier = linear_model.LogisticRegression()\n","    classifier.fit(x_train_count_pca, y_train)\n","    \n","    predictions = classifier.predict(x_valid_count_pca)\n","    \n","    res = res.append(\n","        pd.DataFrame([[str(n_components), accuracy_score(y_valid, predictions)]], columns=['# Components', 'Score'])\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AI8MhtcQ5q0t"},"outputs":[],"source":["res.sort_values('Score', ascending=False, inplace=True)\n","res.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U71rSuKD5q0w"},"outputs":[],"source":["res = pd.DataFrame()\n","\n","for n_components in [100, 300, 500, 1000, 3000, 5000]:\n","    svd = TruncatedSVD(n_components=n_components)\n","    \n","    x_train_count_3 =  tfidf_vec_corp.transform(x_train)\n","    x_valid_count_3 =  tfidf_vec_corp.transform(x_valid)\n","\n","    x_train_count_pca =  svd.fit_transform(x_train_count_3)\n","    x_valid_count_pca =  svd.transform(x_valid_count_3)\n","\n","    classifier = linear_model.LogisticRegression()\n","    classifier.fit(x_train_count_pca, y_train)\n","    \n","    predictions = classifier.predict(x_valid_count_pca)\n","    \n","    res = res.append(\n","        pd.DataFrame([[str(n_components), accuracy_score(y_valid, predictions)]], columns=['# Components', 'Score'])\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cAEccHeM5q0x"},"outputs":[],"source":["res.sort_values('Score', ascending=False, inplace=True)\n","res.head()"]},{"cell_type":"markdown","metadata":{"id":"dnkVNf915q0y"},"source":["Сокращение количества признаков до 3000 увеличило score обоих векторайзеров"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}